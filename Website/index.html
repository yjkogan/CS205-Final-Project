<html lang="en" xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta name="generator" content=
  "HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org" />
  <meta charset="utf-8" />

  <title>CS205 Final Project</title>
  <link rel="stylesheet" href="./css/jquery.ui.all.css" type="text/css" />
  <link rel="stylesheet" href="./css/demos.css" type="text/css" />
  <script src="./js/jquery-1.6.2.js" type="text/javascript">
</script>
  <script src="./js/jquery.ui.core.js" type="text/javascript">
</script>
  <script src="./js/jquery.ui.widget.js" type="text/javascript">
</script>
  <script src="./js/jquery.ui.tabs.js" type="text/javascript">
</script>
  <script type="text/javascript">
//<![CDATA[
        $(function() {
                $( "#tabs" ).tabs();
        });
  //]]>
  </script>
  <style type="text/css">
/*<![CDATA[*/
  li.c1 {list-style: none; display: inline}
  /*]]>*/
  </style>
</head>

<body>
  <div class="centered">
    <div id="header">
      <h1>Similarity Score Generation over Heterogeneous Data</h1>

      <h3>CS205 | Fall 2011<br />
      Kane Hsieh &amp; Yonatan Kogan</h3>
    </div>

    <div class="demo" id="tabs">
      <ul>
        <li><a href="#tabs-1">Problem</a></li>

        <li><a href="#tabs-2">Data</a></li>

        <li><a href="#tabs-3">Design</a></li>

        <li><a href="#tabs-4">Usage</a></li>

        <li><a href="#tabs-5">Performance</a></li>

        <li><a href="#tabs-6">Insights</a></li>

        <li><a href="#tabs-7">Extensions</a></li>

        <li><a href="#tabs-8">Reflection</a></li>

        <li><a href="#tabs-9">Video</a></li>
      </ul>

      <div id="tabs-1">
        <p><img src="img/teacher.jpg" /></p>

        <p>An identified problem in primary and secondary education is that it is often
        necessary for teachers to spend an inordinate amount of time making lesson plans,
        rather than focusing on increasing individual student performance, even though
        the lesson plans have likely already been developed and refined by similar
        teachers. This is because there is no efficient or standardized way to share and
        peer review lesson plans.</p>

        <p><a href="http://betterlesson.com">BetterLesson</a> is a Boston-based startup
        that seeks "to connect educators and help them create, organize, and share their
        curricula." They have collected a large database of teachers, lesson plans, and
        related metadata. In order to help teachers identify relevant lesson plans,
        BetterLesson hopes to direct teachers to other educators who share similar
        content areas and teaching styles.</p>

        <p>This recommendation engine needs to be fast against a increasingly large
        dataset.</p>

        <p>The focus of this project is minimizing the latency that a user of
        BetterLessons experiences when searching for similar teachers and lesson
        plans.</p>
      </div>

      <div id="tabs-2">
        <p><img src="img/betterlesson.png" /></p>

        <p>The data was provided to us by <a href=
        "http://betterlesson.com/">BetterLesson</a>, an organization that seeks to aid
        teachres in collaborating and sharing the best course material. In their own
        words, "We are focused on aggregating and scaling the most innovative content and
        practices from high-performing teachers across the country."</p>

        <p>We were provided with a 120 megabyte CSV in which each row represented an
        individual teacher. The columns provided were: <samp>UID, firstName, state,
        dateCreated, score</samp> (which is BetterData's own metadata 'ranking'),
        <samp>upload</samp> and <samp>download</samp>, (which refer to lesson plan
        contributions), <samp>schoolName,</samp> and <samp>courseNames, unitNames</samp>,
        and <samp>grades</samp>. The last three were lists of strings delimited by the
        '|' character. An example of a row of data is below:</p>

        <p class="samplecode"><samp>13,Alex,MA,9/5/2008 15:58,7115874,456,3142,Roxbury
        Prep Middle School,Social Studies | English / Language Arts | Writing | Listening
        and Speaking,Roxbury Prep 6th Grade English | Brown Social Studies | Grammar and
        Mechanics,Intro to Nouns and Verbs | Formal Writing Assignments | World War II |
        Nouns and Verbs | Nouns and Adjectives | Adjectives and Adverbs | Advertising
        Project | Autobiography Writing Project | How to Write a Business Letter | Time
        For Kids | Commonly Misused Words | Creative Writing Assignments | Writing
        Rubrics | Paragraph Structure | Assorted Grammar | Sentence Types | Figurative
        Language,Sixth grade | Seventh grade | Eighth Grade</samp></p>

        <p>The goal was to use a mapper to generate a "similarity" score between every
        pair of rows in order to recommend the most similar teachers for collaborative
        purposes. To this end, we also generated two variations of the dataset.
        <samp>reshaped_coarsegrained_mapper_user_export.csv</samp> provides the same
        data, but each cell is a 5-tuple to reduce the frequency that data is transferred
        between the devices and mappers/reducers:</p>

        <p class="samplecode">
        <samp>(13,14,15,16,17),(Alex,John,Steve,Kim,Kyle),(MA,MA,NY,MA,TX)... (Seventh
        grade | Eighth Grade, Fifth Grade | Eighth Grade, Sixth Grade, Fifth Grade |
        Sixth Grade, Eighth Grade)</samp></p>

        <p>The second variation of data we created was
        <samp>reshaped_finegrained_mapper_user_export.csv</samp>, in which each row as
        the format <samp>UID,[column name],[value]</samp>. This results in ten rows of
        data per teacher. The purpose of this dataset is so that each mapper has to do
        less work because it only computes the score based on one column; however, more
        mappers are needed to do the full computation. Thus, this dataset is advantageous
        only when mappers are cheap:</p>

        <p class="samplecode"><samp>13,firstName,Alex<br />
        13,state,MA<br />
        13,school,Roxbury Prep Middle School<br />
        ...<br />
        16,name,Kim<br />
        ...<br />
        16,grades,Fifth Grade | Sixth Grade<br />
        ...</samp></p>

        <p>Performance on Amazon EMR small instances are discussed in the
        <b>Performance</b> section.</p>
      </div>

      <div id="tabs-3">
        <p>blah</p>
      </div>

      <div id="tabs-4">
        <p><img src="img/usage.jpg" /></p>

        <p>To run the project, use the following command line arguments:</p>

        <p class="samplecode"><samp>python [LAUNCHER] [MAPPER] [DATASET]</samp></p>

        <p>For example:</p>

        <p class="samplecode"><samp>python mr_launcher.py fine_grained_mr.py
        reshaped_coarsegrained_mapper_user_export.csv</samp></p>

        <p>By default, the script runs locally. With a properly configured
        <samp>.mrjob</samp> file, the flag <samp>-emr</samp> can be used to run the
        script on Amazon EMR. The result of a successful run is the creation of file
        <samp>scoreDict.txt</samp> and a line in the file <samp>perflog.csv</samp>. Lines
        in <samp>perflog.csv</samp> have the following format:</p>

        <p class="samplecode"><samp>python mr_launcher.py fine_grained_mr.py
        reshaped_coarsegrained_mapper_user_export.csv</samp></p>

        <p>By default, the script runs locally. With a properly configured
        <samp>.mrjob</samp> file, the flag <samp>-emr</samp> can be used to run the
        script on Amazon EMR. The result of a successful run is the creation of file
        <samp>scoreDict.txt</samp> and a line in the file <samp>perflog.csv</samp>. The
        file <samp>perfLog.csv</samp> is a log generated by the launcher each time it
        runs the mapper, and each line has the following format:</p>

        <p class="samplecode"><samp>[ROWS COMPUTED],[TOTAL RUNTIME],[AVG
        RUNTIME],[TIMESTAMP]</samp></p>

        <p>The other file, <samp>scoreDict.txt</samp>, is upper-right matrix of
        teacher-teacher similarity scores in a python <samp>dict</samp> format. Each
        entry in <samp>scoreDict.txt</samp> has the following format:</p>

        <p class="samplecode"><samp>(UID1,UID2) : [SIMILARITY SCORE]</samp></p>

        <p>The key format is a tuple, <samp>(UID1,UID2)</samp>, of any two teachers, and
        the value <samp>[SIMILARITY SCORE]</samp> is a score on the scale of 0-1 as
        generated by the functions in <samp>comparisons.py</samp> (see <b>Design</b> for
        more information). The dictionary allows for constant time lookup of the
        similarity of any two teachers, and an <samp><i>n</i>log<i>n</i></samp> lookup of
        the top X similar teachers for any given teacher.</p>
      </div>

      <div id="tabs-5">
        <p>To benchmark our project, we used an artificially inflated dataset with
        something on the order of 6*1<samp>E</samp>6 teachers. This dataset was reshaped
        accordingly in order to benchmark with our coarse mapper and fine mapper, which
        had 1.2*1<samp>E</samp>6 and 6*1<samp>E</samp>7 rows, respectively (see
        <b>Data</b> for more information).</p>

        <p>The following results were obtained using 16 small instances on Amazon
        EMR:</p>

        <ul>
          <li>The serial approach took <b>483 seconds</b>.</li>

          <li>The simplistic approach, in which each mapper generates the teacher's score
          across every data column of another teacher and returns the top 10, took <b>327
          seconds</b>.</li>

          <li>The fine grained apprach, in which each mapper generates the teacher's
          score across one data column of another teacher, took <b>590 seconds</b>.</li>

          <li>The coarse grained apprach, in which each mapper generates the teacher's
          score across every data column of 5 teachers at a time and returns the top 10,
          took <b>1087 seconds</b>.</li>
        </ul>

        <p><img class="fig" src="img/fig1.jpg" /></p>

        <p>It is apparent here that our simplistic mapper approach took the least time to
        generate all the scores for one teacher row across the large dataset. The
        implication here is that it is more costly for a mapper to do a lot of work than
        it is to pass more data.</p>

        <p>We also benchmarked all of our implementations against datasets of various
        sizes to see how performance varies across the size of the input data. The
        following results were obtained using 16 small instances on Amazon EMR:</p>

        <p><img class="fig" src="img/fig2.jpg" /></p>
		
		<p>As expected, the advantage of using map-reduce manifests as the dataset gets larger. This is apparent with our simple mapper, and the fine mapper performance is approaching that of the serial mapper for our full dataset. Inferring from the trend lines, we can expect that the fine mapper will outperform the serial implementation if we double the datasize again.</p>
		<p>We were confused and concerned by the degenerating performance of the coarse grain mapper, whose run time increased exponentially every time we doubled the dataset size. This was worse than the serial implementation. We hypothesized that this was the result of caching and memory limitations on the anemic small EC2 instances that only affected our coarse mapper because of its much larger line size. To explore this hypothesize, we fixed the number of instances at 16 but launched our coarse mapper on a large EC2 instance.</p>

		<p>We also tried a different approach, in which we use map-reduce to initially
        build the entire matrix of teacher-teacher similarity scores and save it as a
        dictionary with the entry format <samp>(id1,id2) : [SCORE]</samp>. The dictionary
        allows for constant time lookup of the similarity of any two teachers, and an
        <samp><i>n</i>log<i>n</i></samp> lookup of the top X similar teachers for any
        given teacher. We tested our upper-triangle dictionary builder (see <b>Design</b>) on 20 rows, which built a 6 * 1<samp>E</samp>6 by 20 matrix. This ran in 18760 seconds, or an average of 938 seconds per row; however, this time was largely dominated by the time required to launch an EC2 instance for every row. The implication here is that a <samp>for</samp> loop to launch mappers for the rows is a nonideal design.</p>

        <p>CONCLUSIONS - do not build entire dictionary at once initially (unless you
        have super fast computers). Rather, save each dictionary row on the intial check
        so future comparisons can be done in constant time.</p>
      </div>

      <div id="tabs-6">
        <p>The most significant thing we learned from this project is that the map-reduce
        framework is not very flexible. It does not lend itself well to running
        two-variable computations when both variables are indepedendent and unknown in
        advance. This made it very difficult to fill a matrix of similiarty scores
        without running mutliple map-reduce jobs.</p>

        <p>One way to address this is to reshape the input data, but in our case this
        would have led to a quadratic increase in space usage to store the input
        data.</p>

        <p>We tried having each mapper open its own version of the input data, but this
        was also very time and space costly, especially when we were waiting on Amazon
        EC2.</p>
      </div>

      <div id="tabs-7">
        <p><img src="img/extensions.jpg" /></p>

        <p>One extension is to refine the code that generates the upper triangular matrix
        of teacher-teacher scores such that every loop iteration that launches mappers
        reuses the same EC2 instances as the previous iteration rather than starting new
        ones, since waiting for instances to start was the majority of the total runtime
        for the code that built the dictionary</p>

        <p>A better implementation would be to launch each row computation of the matrix
        in parallel, rather than waiting for the loop iterations. An even better
        implementation would be to load-balance row computations across jobs in parallel
        such as the following:</p>

        <p><img class="fig" src="img/matrix_divvy.jpg" /></p>

        <p>This would make our dictionary buildling time comparable to the simple mapping
        time (see <b>Performance</b>), assuming we could launch <samp>n/2</samp> jobs,
        where <samp>n</samp> is the number of rows in the matrix. Otherwise, with
        <samp>t</samp> jobs, we can build the entire dictionary in the same time as the
        simple mapper times a factor of <samp>(n/2*t)</samp>.</p>

        <p>Another way to increase the performance of our project would be to find the
        bucket size for the coarse mapper to optimize the cost of computation versus the
        cost of passing data. Our project used a rows of 5-tuples in the coarse mapper,
        which did not give us performance better than the simple or fine mapper (see
        <b>Performance</b>). Without testing more we don't know if this optimum bucket
        lies between our fine and simple mapper, between our simpmle and coarse mapper,
        or even larger than our coarse mapper.</p>

        <p>This was not within the scope of the project, but our score generator
        functions are very rudimentary. Ideally the comparison on rows such as
        <samp>subjects</samp> and <samp>courses</samp> would involve some sophisticated
        natural language processing which could run in parallel as well since there are
        no dependencies between elements in the matrix.</p>
      </div>

      <div id="tabs-8">
        <p>Mauris eleifend est et turpis. Duis id erat. Suspendisse potenti. Aliquam
        vulputate, pede vel vehicula accumsan, mi neque rutrum erat, eu congue orci lorem
        eget lorem. Vestibulum non ante. Class aptent taciti sociosqu ad litora torquent
        per conubia nostra, per inceptos himenaeos. Fusce sodales. Quisque eu urna vel
        enim commodo pellentesque. Praesent eu risus hendrerit ligula tempus pretium.
        Curabitur lorem enim, pretium nec, feugiat nec, luctus a, lacus.</p>

        <p>Duis cursus. Maecenas ligula eros, blandit nec, pharetra at, semper at, magna.
        Nullam ac lacus. Nulla facilisi. Praesent viverra justo vitae neque. Praesent
        blandit adipiscing velit. Suspendisse potenti. Donec mattis, pede vel pharetra
        blandit, magna ligula faucibus eros, id euismod lacus dolor eget odio. Nam
        scelerisque. Donec non libero sed nulla mattis commodo. Ut sagittis. Donec nisi
        lectus, feugiat porttitor, tempor ac, tempor vitae, pede. Aenean vehicula velit
        eu tellus interdum rutrum. Maecenas commodo. Pellentesque nec elit. Fusce in
        lacus. Vivamus a libero vitae lectus hendrerit hendrerit.</p>
      </div>

      <div id="tabs-9">
        <p><iframe width="420" height="315" src=
        "http://www.youtube.com/embed/dQw4w9WgXcQ" frameborder="0" allowfullscreen=
        ""></iframe></p>
      </div>
    </div>
  </div>
</body>
</html>