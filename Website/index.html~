<html lang="en" xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta name="generator" content=
  "HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org" />
  <meta charset="utf-8" />

  <title>CS205 Final Project</title>
  <link rel="stylesheet" href="./css/jquery.ui.all.css" type="text/css" />
  <link rel="stylesheet" href="./css/demos.css" type="text/css" />
  <script src="./js/jquery-1.6.2.js" type="text/javascript">
</script>
  <script src="./js/jquery.ui.core.js" type="text/javascript">
</script>
  <script src="./js/jquery.ui.widget.js" type="text/javascript">
</script>
  <script src="./js/jquery.ui.tabs.js" type="text/javascript">
</script>
  <script type="text/javascript">
//<![CDATA[
        $(function() {
                $( "#tabs" ).tabs();
        });
  //]]>
  </script>
  <style type="text/css">
/*<![CDATA[*/
  li.c1 {list-style: none; display: inline}
  /*]]>*/
  </style>
  <style type="text/css">
/*<![CDATA[*/
  pre.c1 {font-size: 12}
  /*]]>*/
  </style>
</head>

<body>
  <div class="centered">
    <div id="header">
      <h1>Similarity Score Generation over Heterogeneous Data</h1>

      <h3>CS205 | Fall 2011<br />
      Kane Hsieh &amp; Yonatan Kogan</h3>
    </div>

    <div class="demo" id="tabs">
      <ul>
        <li><a href="#tabs-1">Problem</a></li>

        <li><a href="#tabs-2">Data</a></li>

        <li><a href="#tabs-3">Design</a></li>

        <li><a href="#tabs-4">Usage</a></li>

        <li><a href="#tabs-5">Performance &amp; Insights</a></li>

        <li><a href="#tabs-6">Extensions</a></li>

        <li><a href="#tabs-7">Reflection</a></li>

        <li><a href="#tabs-9">Video</a></li>
      </ul>

      <div id="tabs-1">
        <p><img src="img/betterlesson.png" /></p>

        <p>An identified problem in primary and secondary education is that it is often
        necessary for teachers to spend an inordinate amount of time making lesson plans,
        rather than focusing on increasing individual student performance. One way to help teachers spend more time on teaching is to
        give them access to the many high quality lesson plans that have already been developed and refined by similar
        teachers.</p>

        <p><a href="http://betterlesson.com">BetterLesson</a> is a Boston-based startup
        that seeks "to connect educators and help them create, organize, and share their
        curricula." They have collected a large database of teachers, lesson plans, and
        related metadata. In order to help teachers identify relevant lesson plans,
        BetterLesson hopes to direct teachers to other educators who share similar
        content areas and teaching styles.</p>

        <p>This recommendation engine needs to be fast against a increasingly large
        dataset.</p>

        <p>The focus of this project is minimizing the latency that a user of
        BetterLessons experiences when searching for similar teachers and lesson
        plans.</p>
      </div>

      <div id="tabs-2">
        <p><img src="img/betterlesson.png" /></p>

        <p>The data was provided to us by <a href=
        "http://betterlesson.com/">BetterLesson</a>, an organization that seeks to aid
        teachres in collaborating and sharing the best course material. In their own
        words, "We are focused on aggregating and scaling the most innovative content and
        practices from high-performing teachers across the country."</p>

        <p>We were provided with a 12.0 megabyte CSV in which each row represented an
        individual teacher. The columns provided were: <samp>UID, firstName, state,
        dateCreated, score</samp> (which is BetterData's own metadata 'ranking'),
        <samp>upload</samp> and <samp>download</samp>, (which refer to lesson plan
        contributions), <samp>schoolName,</samp> and <samp>courseNames, unitNames</samp>,
        and <samp>grades</samp>. The last three were lists of strings delimited by the
        '|' character. An example of a row of data is below:</p>

        <p class="samplecode"><samp>13,Alex,MA,9/5/2008 15:58,7115874,456,3142,Roxbury
        Prep Middle School,Social Studies | English / Language Arts | Writing | Listening
        and Speaking,Roxbury Prep 6th Grade English | Brown Social Studies | Grammar and
        Mechanics,Intro to Nouns and Verbs | Formal Writing Assignments | World War II |
        Nouns and Verbs | Nouns and Adjectives | Adjectives and Adverbs | Advertising
        Project | Autobiography Writing Project | How to Write a Business Letter | Time
        For Kids | Commonly Misused Words | Creative Writing Assignments | Writing
        Rubrics | Paragraph Structure | Assorted Grammar | Sentence Types | Figurative
        Language,Sixth grade | Seventh grade | Eighth Grade</samp></p>

        <p>The goal was to use a mapper to generate a "similarity" score between every
        pair of rows in order to recommend the most similar teachers for collaborative
        purposes. To this end, we also generated two variations of the dataset.
        <samp>reshaped_coarsegrained_mapper_user_export.csv</samp> provides the same
        data but each row is a concatenation of 5 teachers:</p>

        <p class="samplecode"><samp>13,Alex,MA ... Seventh grade | Eighth Grade,14,Matt,
        MA ... 17,Becky ...</samp></p>

        <p>We also created similar files with 2 teachers per row and 10 teachers per
        row.</p>

        <p>The second variation of data we created was
        <samp>reshaped_finegrained_mapper_user_export.csv</samp>, in which each row has
        the format <samp>UID,[column name],[value]</samp>. This results is 12 rows of
        data per teacher. The purpose of this dataset is so that each mapper has to do
        less work because it only computes the score based on one column; however, more
        mappers are needed to do the full computation. Thus, this dataset is advantageous
        only when mappers are cheap:</p>

        <p class="samplecode"><samp>13,firstName,Alex<br />
        13,state,MA<br />
        13,school,Roxbury Prep Middle School<br />
        ...<br />
        16,name,Kim<br />
        ...<br />
        16,grades,Fifth Grade | Sixth Grade<br />
        ...</samp></p>

        <p>For benchmarking, we copied the provided file repeatedly in order to generate
        a suitably large dataset (on the order of 1 GB). We then reshaped this into the
        appropriate data formats to run with the other mappers. Performance on Amazon EMR
        are discussed in the <b>Performance</b> section.</p>
      </div>

      <div id="tabs-3">
		
		<p><img src="img/design.jpg" /></p>
	  
        <h2>mr_launcher2.py</h2>

        <p>We wrote a file called mr_launcher.py so that we could easily hard code
        varying inputs into our files that ran map-reduce. This is because it is very
        hard to give map-reduce two inputs without coding them both into the data, but
        since we were always comparing the same teacher against others we could hard code
        it. Mr_launcher enabled us to do this easily, as well as hard-code a different
        relationship between column names and colmun numbers if we were to use a
        different dataset in the future. We wrote this file as an extension to
        mr_launcher so that we could easily kick off map reduce jobs for multiple
        teachers in succession. Since we are hard coding each teacher name, the file to
        run changes every iteration. This code creates a new temp file every iteration
        with a different teacher hard-coded in.</p>
        <pre class="samplecode c1">
<samp>
 import subprocess,time,sys,csv
 import string as st
 # We wrote a custom parser for the command line.
 # See usage for more information
 from myparser import parser

 # First we open the script we are going to run on 
 # loop and store it as a string so that we can easily
 # change it.
 file = open(args.scripttorun,'r')
 filetext =  file.read()
 file.close()
 # Then we load up the database we are going to 
 # hard-code teachers from.
 inputfile = csv.reader(open(args.inputfile,'r'))
 # We also want a datatype in which to store all 
 # the comparisons that come back. We want to be 
 # able to easily check if a comparison has been done already
 # so we will store each comparison as a tuple of teacher ids 
 # and an associated score. This allows for some interesting 
 # extensions, discussed elsewhere.
 scoreDict = {}
 
 ...
        
 # Here is where we iterate over every teacher in the database 
 # of teachers we want to compare
   for row in inputfile:
                        
    # For each teacher, get their ID (it's in the first cell). 
    # Then use string replace to replace the placeholder text 
    # with that particular teachers data
    # e.g. write into the file "teacherstring = " teacher's data. 
    # Then save the file as a temporary script.
    currentID = eval(row[0])
    teacherstring = 'teacherlist = ' + str(row)
    tempfiletext = st.replace(filetext,'#teacherlist_placeholder#',\
                              teacherstring)


    # If the -emr flag is given, then this runs the given script 
    # (repeatedly) on emr. The number of mapper tasks is set to 4 because 
    # the limit on the number of instances and the delay
    # of EC2 from recognizing that you are no longer using your instances
    # prevents you from quickly booting up another tasks. Therefore we 
    # set the value far below our limit.
    # With a different account (or a greater willingness to have the 
    # script fail sometimes) this could be increased.
    if(emr):   
       proc = subprocess.Popen(['python','tempscript.py',args.database,'-r',\
            'emr','--jobconf','mapred.map.tasks=4'],stdout=subprocess.PIPE)
    # Otherwise we just run it locally.
    else:
       proc = subprocess.Popen(['python','tempscript.py',args.database],\
                stdout=subprocess.PIPE)
                     
    # matrix_mr.py always returns a list of lists that we can find using a regex.
    # Using this regex we can read the results of each run in from
    # standard out and add it to our dictionary.
    while True:
                line = proc.stdout.readline()
                if line != '':
                        # gets the list of comparisons from stdout w regex
                        result = eval(re.search('\\t(.*)\\n',line).group(0))
                        for k,v in result: # k,v is teacherID, score
                                keyTuple = tuple(sorted([k,currentID]))
                                if (keyTuple not in scoreDict):
                                        scoreDict[keyTuple] = v # set new score
        </samp>
</pre>

        <p>We also created a way to limit the number of iterations or to prevent it from
        running another iteration. When it's all done, we write out to a file the total
        amount of time as well as the average amount of time it took to do each run.</p>

        <h2>matrix_mr.py</h2>

        <p>This file is the basic map-reduce job that we ran using mr_launcher2. There
        are a number of design choices of note. The first is that we hard coded the
        column number for each field. Ideally, the code could just read this from the top
        of the file but it is hard to tell map-reduce not to distribute and run the first
        row of the input file. We also made a dictionary relating field name and the
        function that calculates the similarity for that field so that we can access the
        function directly from a dictionary index. With regard to efficiency, we set an
        invariant that teachers should only be compared to other teachers with higher ID
        numbers. This reduces our runtime by about a factor of 2 because we now only fill
        half of the matrix of similarity scores. Since the scores are symmetric across
        the diagonal, this still gets us all the data we want. Furthermore, in this
        script we only return the 10 most similar teachers. We tried running a script
        that passed the information about every teacher and had to kill it after an hour.
        We think this is because of the high cost of passing so much data in a map-reduce
        framework. It might also be because mr_job's mapper_final doesn't play nice was
        very large dictionaries or lists.</p>

        <p>All of the comparison functions we made are very rudimentary, since we spent
        the majority of our time experimenting with different designs for the algorithm.
        Also of note is that before we ran code on EMR we copied all of the comparisons
        into the file we were going to run on EMR, since trying to get another file up to
        EMR to be imported proved to be pretty difficult despite mr_job support.</p>
        <pre class="samplecode c1">
<samp>
 # import MRJob class
 from mrjob.job import MRJob
 from collections import defaultdict

 # This is the string we replace with the data for the 
 # particular teacher we want to run.
 #teacherlist_placeholder# 

 # Dictionary of the the column number associated with each field
 coldict={'SchoolName':7,'State':2,'Grades':11,'TeacherID':0,\
          'TeacherName':1,'DateCreated':3,'Score':4,'Uploads':5,\
          'Downloads':6,'Subjects':8,'Courses':9,'Units':10}

 # Dictionary so that we can find the correct function 
 # simply from the name of the field
 funcs = {'SchoolName':compSchool,'State':compState,\
          'Grades':compGrades,'Subjects':compSubjects,
          'Courses':compCourses,'Units':compUnits}


 # get_score is abstracted as a separate function so that it can 
 # be easily changed to incorporate a better system of similarity 
 # score generation. We track both the total score and the number
 # of meaningful comparisons, since some teachers do not have data 
 # for particular columns. In this case we do not count it as a 
 # comparison. A better similarity score system would weigh this
 # as a detriment to the similarity but not as heavily as if the 
 # two teachers were certifiable different in that field.
 def get_score(teacher,tocompare):
    #keeps track of the total score
    scoreval =0
    #keeps track of the number of meaningful comparisons
    num_cats_compared = 0
    #iterates through the different columns we want to compare
    for c,v in coldict.iteritems():
       score = None
       if c in funcs:
          score = funcs[c](teacher[v],tocompare[v])
       #If a meaningful comparison happened, update the score trackers
       if(score != None):
          scoreval += score
          num_cats_compared += 1

    #Returns a tuple of the score and the number of meaningful comparisons
    return (round(scoreval,5),num_cats_compared)

class MySimTeachers(MRJob):
    def __init__(self,*args,**kwargs):
        super(MySimTeachers,self).__init__(*args,**kwargs)
        # List to keep track of the teachers we have compared
        self.comparedts = []
        # The teacher we are comparing
        self.teacher = teacherlist

    def mapper(self, key, value):
        ...
        # If the teacher has a lower ID number than the current teacher we are comparing, 
        # just return without adding anything to the list of compared teachers.
        if (self.teacher[0] &gt;= row[0]):
                return
        score = get_score(self.teacher,row)
        # Once we have the score, add it to the list of teachers that have been compared.
        # If no meaningful comparisons happened (meaning score[1] is zero), add nothing
        try:
            self.comparedts.append((int(row[0]),score[0]/score[1]))
        # Furthermore, our dataset was not very clean so occasionally python's 
        # list/dictionary datatype wouldn't want to accept a particular input. 
        # In this case no action is required, and printing to stdout
        # breaks map-reduce so instead just pass. We can change this during debugging.
        except ZeroDivisionError, ValueError:
            pass
    
    def mapper_final(self):
        # yield the top ten most similar teachers
        # The score is the 1 index in the tuple, so make that the key
        # Then sort so that the front of the list is the most similar teachers
        # Only pass the top 10 because a) we aren't going to use more than that
        # and b) because it is very costly on the map-reduce framework to pass
        # lots of data
        yield None, sorted(self.comparedts,key=(lambda x: x[1]),reverse=True)[:10]

    # override pre-defined reducer by creating a generator
    # with the default name (reducer)
    def reducer(self, key, values):
        # Accumulate all the most similar teachers in one list and select the
        # top 10 of them. We use the same approach as the mapper_final
        allcompared = []
        for value in values:
            allcompared.extend(value)
        yield key, sorted(allcompared,key=(lambda x: x[1]),reverse=True)[:10]

        </samp>
</pre>

        <h2>foremr_coarsegrained.py</h2>

        <p>The file that ran on the coarse grained format of our data behaved exactly the
        same as the file matrix_mr except that instead of doing one teacher comparison,
        it loops over all X teachers that it reads in.</p>
        <pre class="samplecode c1">
<samp>
        ...
        num_teachers = len(row)/rowlen
        # calculate the score
        for t in xrange(num_teachers):
            score = get_score(self.teacher,row[t*rowlen:(t+1)*rowlen])
        ...
   </samp>
</pre>

        <h2>foremr_finegrained.py</h2>

        <p>The file that ran on the fine grained format of our data had a slightly
        different get_score function because each row has the field information inside
        it. We did this because it is unclear how to determine what kind of input
        something is based on the string. The other small change is that the fine grained
        mapper keeps track of all the scores done for a particular teacher we are
        comparing by adding each score to a dictionary. Mapper_final then sums all these
        scores and yields the total score for the teacher being compared</p>
        <pre class="samplecode c1">
<samp>
        def mapper(self,key,value):
        ...
           if(score != None):
              self.scores[id].append(score[0])
              self.numcompared[id]+=score[1]
        
        def mapper_final(self):
           for t,scores in self.scores.iteritems():
           # This sums and yields the score for each teacher we compared the
           # hard-coded teacher against. Note the different data format which
           # is more human readable but harder to parse. This script would
           # not easily work with mr_launcher2, but we found that it was
           # slower than the simple version and so did not attempt to
           # adapt it so it could be run more frequently.
           score = reduce((lambda x,y:x+y),scores,0)
           if score != 0:
              yield t,score/len(scores)
        ...
                </samp>
</pre>

		<h2>foremr_combiner.py</h2>
		<p>This is exactly the same code as <samp>matrix_mr.py</samp> except that it uses a combiner
		instead of a <samp>mapper_final</samp>. The change was made to experiment with slightly different
		algorithmic approach.</p>
		
		<h2>foremr_probability.py</h2>
		<p>This is also the same as <samp>matrix_mr.py</samp>, but only actually runs comparisons on 10% of the teachers.
		The thought behind this is that we can approximate the best possible result this way but only do a fraction of the work.
		Since we are not interested in finding the absolute most similar teachers, but rather some similar teachers this will still get
		us the information we need.</p>
		
		<h2>foremr_headts.py</h2>
		<p>This code opens a database of teachers for whom we know the most similar teachers, and compares the current teacher_row
		against all of those teachers. It then goes through the lists of similar teachers for each teacher that the current_row is similar to
		and builds a list of teachers we probably want to compare the current teacher_row against. It then only runs comparisons on teachers that
		appear in that list. The idea behind this is that if A is similar to B, and B is similar to C, D and E, then A is probably similar to C, D, and E
		and not similar to F and G, who are not similar to B. This reduces the number of comparisons we need to make and so speeds up the code.</p>
      </div>

      <div id="tabs-4">
        <p><img src="img/usage.jpg" /></p>

        <p>To run the project, use the following command line arguments:</p>

        <p class="samplecode"><samp>python [LAUNCHER] [MAPPER] [DATASET_TO_COMPARE_WITH]
        [DATASET_TO_READ_FROM]</samp></p>

        <p>By default, the script runs locally. With a properly configured
        <samp>.mrjob</samp> file, the flag <samp>-emr</samp> can be used to run the
        script on Amazon EMR. For example:</p>

        <p class="samplecode"><samp>python mr_launcher2.py matrix_mr.py
        s3://temp-205/large_dataset.csv large_dataset.csv -emr</samp></p>

        <p>To change the number of mapper instances you need to go into mr_launcher2.py.
        While there, you can also set a limit on the number of times to launch a job on
        EMR.</p>

        <p>The result of a successful run is the creation of file
        <samp>scoreDict.txt</samp> and a line in the file <samp>perflog.csv</samp>. Lines
        in <samp>perflog.csv</samp> have the following format:</p>

        <p class="samplecode"><samp>[ROWS COMPUTED],[TOTAL RUNTIME],[AVG
        RUNTIME],[TIMESTAMP]</samp></p>

        <p>The other file, <samp>scoreDict.txt</samp>, is an upper-right matrix of
        teacher-teacher similarity scores in a python <samp>dict</samp> format. Each
        entry in <samp>scoreDict.txt</samp> has the following format:</p>

        <p class="samplecode"><samp>(UID1,UID2) : [SIMILARITY SCORE]</samp></p>

        <p>The key format is a tuple, <samp>(UID1,UID2)</samp>, of any two teachers, and
        the value <samp>[SIMILARITY SCORE]</samp> is a score on the scale of 0-1 as
        generated by the functions in <samp>comparisons.py</samp> (see <b>Design</b> for
        more information). The dictionary allows for constant time lookup of the
        similarity of any two teachers, and an <samp><i>n</i>log<i>n</i></samp> lookup of
        the top X similar teachers for any given teacher.</p>

        <p>You can also just directly run one of the scripts in the typical mr.job way
        using the following form:</p>

        <p class="samplecode"><samp>python [MAPPER] [-r emr] [--jobconf
        mapred.red.tasks=n] [DATASET_TO_COMPARE_WITH] &gt; [OUTPUT_FILE]</samp></p>

        <p>However, you need to be careful that you give mappers the correctly formatted
        data file.</p>
        
        <p>The files <samp>serial_example.py</samp>, <samp>n2_inside_mr.py</samp> and <samp>foremr_headts.py</samp> all need to be run
        locally and need some hard coded values to be altered in order to run properly. They are run with the following command line arguments:</p>

			<p class="samplecode"><samp>python [MAPPER] [DATABASE] e.g. python foremr_headts.py user2_export.csv</samp></p>

			<p><samp>serial_example.py</samp> needs the path to the database hard-coded to the variable filename</p>

			<p><samp>n2_inside_mr.py</samp> needs the path to the database hard-coded to the variable filename.</p>

			<p><samp>foremr_headts.py</samp> needs the path to the database of pre-computed teachers and the list of results 
			from that computation. The output of <samp>n2_inside_mr.py</samp> can serve as this list as it is the correct format. 
			Running <samp>n2_inside_mr.py</samp> on a subset of the database was how we generated our list.</p>
      </div>

      <div id="tabs-5">
        <p><a href="http://www.up-video.com/uploads/thumbs/vnyqrddokrdzzysn.jpg"><img src="img/performance.jpg" /></a></p>

        <p>To benchmark our project, we used an artificially inflated dataset with
        something on the order of 6<samp>E</samp>6 teachers. This dataset was reshaped
        accordingly in order to benchmark with our coarse mapper and fine mapper, which
        had 1.2<samp>E</samp>6 and 6<samp>E</samp>7 rows, respectively (see
        <b>Data</b> for more information).</p>

        <p>The following results were obtained using 16 small instances on Amazon
        EMR:</p>

        <ul>
          <li>The serial approach took <b>483 seconds</b>.</li>

          <li>The simplistic approach, in which each mapper generates the teacher's score
          across every data column of another teacher took <b>327
          seconds</b>.</li>

          <li>The fine grained apprach, in which each mapper generates the teacher's
          score across one data column of another teacher, took <b>590 seconds</b>.</li>

          <li>The coarse grained apprach, in which each mapper generates the teacher's
          score across every data column of 5 teachers at a time
          took <b>1087 seconds</b>.</li>
          
          <li>Both the simplistic and coarse approaches only passed the 10 most similar
          teachers on to the reducer, in order to reduce the cost of I/O.</li>
        </ul>

        <p><img class="fig" src="img/fig1.jpg" /></p>

        <p>It is apparent here that our simplistic mapper approach took the least time to
        generate all the scores for one teacher row across the large dataset. The
        implication here is that it is more costly for a mapper to do a lot of work than
        it is to pass more data. However, we will see later that the data from the coarse grained mapper is misleading.</p>

        <p>We also benchmarked all of our implementations against datasets of various
        sizes to see how performance varies across the size of the input data. The
        following results were obtained using 16 small instances on Amazon EMR:</p>

        <p><img class="fig" src="img/fig2.jpg" /></p>

        <p>As expected, the advantage of using map-reduce manifests as the dataset gets
        larger. This is apparent with our simple mapper, and the fine mapper performance
        is approaching that of the serial mapper for our full dataset. Inferring from the
        trend lines, we can expect that the fine mapper will outperform the serial
        implementation if we double the datasize again.</p>

        <p>We were confused and concerned by the degenerating performance of the coarse
        grain mapper, whose run time increased exponentially every time we doubled the
        dataset size. This was worse than the serial implementation. We hypothesized that
        this was the result of caching and memory limitations on the anemic small EC2
        instances that only affected our coarse mapper because of its much larger line
        size. To explore this hypothesis, we fixed the number of instances at 16 and
        launched all the mappers with the full dataset on a large EC2 instance.</p>

        <p><img class="fig" src="img/fig4.jpg" /></p>

        <p>As you can see from the results, the simple and fine mappers experienced
        performance gains of about 50%, which can be attributed to the improved I/O
        throughput and increased computing power of the large EC2 instances; however, the
        coarse mapper experienced a 75% perfomance increase with the same computing bump,
        supporting our hypothesis that the exponentially poor coarse mapper performance
        on the small instance was a result of hardware constraints, particularly in
        caching and memory.</p>

        <p>To further test the hypothesis, we also halved and doubled the size of each
        row in the coarse mapper from a 5-tuple to 2-tuple and a 10-tuple of teacher
        data, respectively, while fixing all other data and compute parameters:</p>

        <p><img class="fig" src="img/fig5.jpg" /></p>

        <p>We see that halving the data size of each row passed to the mapper (and
        consequently increasing the frequency of data passing between machines) results
        in decreased performance on the large EC2 instance when going from 5 to 2
        teachers, but does not change performance when going from 10 to 5 teachers. We
        also ran these calculations with extra large machines, and saw them all level out
        at about 220 seconds. This suggests that the dominant factor in performance,
        given a suitably powerful machine with regard to RAM and processing power, is the
        I/O performance. Since the row with two teachers requires data to be passed more
        often, improving the I/O performance by running that dataset on an extra large
        machine nomalized the time it took to get results. Extrapolating from our initial
        data, run on small instances, we would have expected the opposite of this result, which
        further supports the conclusion that the coarse mapper was being hindered by
        memory limitations rather than anything different in the way it ran or the way
        data was being passed.</p>

        <p><img class="fig" src="img/fig3.jpg" /></p>

        <p>The above chart is identical to the earlier figure except that the instances
        launched were all large instances instead of small ones. The axes are fixed for
        easy comparison to the above figure. We still see that the simple approach runs
        fastest but the difference is much less dramatic. The fine approach is likely
        still suffering from the increased amount of data that needs to be passed to run
        it. The coarse approach is running slower as well, which is somewhat surprising
        given that it is passing and reading less data than the simple solution. This
        suggests that the naive approach is not pushing up against any I/O limitations
        and so there is no improvement to reducing the number of times data is being
        passed. The difference is likely the overhead of parsing large rows and sorting
        larger lists. We expect that we would see an even larger speedup given a larger dataset.</p>

        <p><img class="fig" src="img/fig6.jpg" /></p>

			<p>Another thing we tired was replacing the mapper_final with a combiner. As you can see
			above, this did not work very well. On the half dataset it ran okay but on the full GB dataset
			the performance was terrible. Since our understanding of combiners is that they are basically
			mapper_finals but with different syntax, it is difficult to explain this result. Our best guess
			is that data does need to move between the mapper and the combiner and so there is a large increase
			in the data that needs to get passed. In the mapper_final version, each mapper only passes 10 results
			on to the reducer but if the mapper passes to the combiner then every single comparison gets passed.</p>

        <p>We also tried a different approach, in which we use map-reduce to initially
        build the entire matrix of teacher-teacher similarity scores and save it as a
        dictionary with the entry format <samp>(id1,id2) : [SCORE]</samp>. The dictionary
        allows for constant time lookup of the similarity of any two teachers, and an
        <samp><i>n</i>log<i>n</i></samp> lookup of the top X similar teachers for any
        given teacher. We tested our upper-triangle dictionary builder (see
        <b>Design</b>) on 20 rows, which built a 6<samp>E</samp>6 by 20 matrix. This
        ran in 18760 seconds, or an average of 938 seconds per row; however, this time
        was largely dominated by the time required to launch an EC2 instance for every
        row. The implication here is that a <samp>for</samp> loop to launch mappers in
        EC2 for the rows is a nonideal design. Ideally, we would launch the calculation
        for each row in parallel.</p>

        <p>Based on the above, we estimate the improvement we would expect to by
        calculating only the upper triangle of the matrix rather than the entire matrix.
        For the simplistic approach, we would expect that comparing every teacher to
        every other teacher would take <samp>327 seconds * num_teachers</samp>. However,
        using the data we collected from running the simplistic approach on smaller
        versions of the dataset we can approximate the average runtime for calculating only the upper triangular matrix. Assuming a
        roughly linear relationship we get that the average runtime per teacher would be
        about <samp>(105+327)/2 = 216</samp> giving a total runtime of <samp>216 seconds
        * num_teachers</samp>. This is less than the factor of two we expected, probably
        because of the overhead associated with distributing data to the mappers and
        passing data between mapper and reducer. Since we only pass the top 10 teachers,
        the reduction in the number of teachers we are comparing against does not improve
        data passing between mapper and reducer, thus a smaller increase than we might
        expect.</p>
        
        <p>We also wrote code that would calculate scores for the entire triangular matrix
        in one map-reduce job, thus reducing the heavy cost of starting up new instances for
        each row. However, EMR did not play nice with the python <samp>open(file)</samp> command
        and so we were unable to get benchmarked data. For the same reason, we were unable to get
      	meaningful results for our mapper that only compared the hard-coded teacher against
      	a subset of the total database. For details on the selection of that subset see the header
      	<samp>foremr_headts.py</samp> on the <b>Design</b> page.</p> 

        <p>Our combined results show that increasing the number of mapper instances
        dramatically increased performance when the number of instances was at or below
        the number of available cores. This confirms our expectation.</p>

        <p>Examining all of the results above, we suggest a memoized approach to
        dictionary buildling. Starting from scratch, the first time a teacher comparison
        is done, the resulting dictionary row should be stored. This avoids the large
        initial cost of building the dictionary, but allows for future comparisons to be
        done very quickly. One caveat is that the entire matrix would have to be
        calculated one row at a time, rather than the faster upper-right, because that
        approach prevents you from getting comparisons to any teacher with a lower
        <samp>UID</samp> if that teacher's row computation has not been run yet.</p>
      </div>

      <div id="tabs-6">
        <p><img src="img/extensions.jpg" /></p>

        <p>One extension is to refine the code that generates the upper triangular matrix
        of teacher-teacher scores such that every loop iteration that launches mappers
        reuses the same EC2 instances as the previous iteration rather than starting new
        ones, since waiting for instances to start was the majority of the total runtime
        for the code that built the dictionary</p>

        <p>A better implementation would be to launch each row computation of the matrix
        in parallel, rather than waiting for the loop iterations. An even better
        implementation would be to load-balance row computations across jobs in parallel
        such as the following:</p>

        <p><img class="fig" src="img/matrix_divvy.jpg" /></p>

        <p>This would make our dictionary buildling time comparable to the simple mapping
        time (see <b>Performance</b>), assuming we could launch <samp>n/2</samp> jobs,
        where <samp>n</samp> is the number of rows in the matrix. Otherwise, with
        <samp>t</samp> jobs, we can build the entire dictionary in the same time as the
        simple mapper times a factor of <samp>(n/2*t)</samp>.</p>

        <p>We attempted to implement a map-reduce framework for which each mapper would
        run one teacher on the entire database, but couldn't figure out how to do this
        without loading the entire file into the script. It is very costly to upload the
        entire file so we did not want to approach the problem that way. We tried having
        each mapper open the database on the server, but ran into two separate issues. We
        couldn't have this as part of the "init" function for the mapper since python
        would correctly catch that no file "s3://" existed locally, so we tried having it
        open inside the call to mapper, but this also seemed to run into issues, probably
        associated with continually opening and closing the file on the server. It ran
        correctly locally so it's unclear exactly what was causing the issue. This file is
        <samp>n2_inside_mr.py</samp>.</p>

        <p>We were hoping to use this in order to create a number of interesting
        extensions. Doing so would dramatically reduce
        the amount of time it takes to compute the rows for the first 1000 or so teachers. From
        this, we can dramatically reduce the runtime for other teachers by
        comparing them first to these 1000 and storing those similar to the teacher in question in a list. Then we can aggregate all
        the teachers similar to some teacher in this list, and compare the teacher in question only to these teachers.
        However, the map-reduce framework does not lend
        itself well to reshaping data, at least not on EMR, because EC2 does not like us
        writing and reading data locally.</p>
        
        <p>The file <samp>foremr_headts.py</samp> is the prototype of this extension. It works
        locally but not on EMR. Testing was done using a toy-database. Support for the usefulness
        of this approach comes from our results, which showed that running this file gave the
       	same scores as running the simple mapper. This would probably not be true on a larger database
       	but it suggests that such an approach could be useful and successful. Unfortunately, because of 
   		the small size of the dictionary and the unrealistic nature of running locally, we do not have
   		meaningful benchmarks. The limited data we have suggests that this approach runs between 6 and 10
   		times faster. This would probably be lower when data passing is necessary</p>

        <p>Another way to increase the performance of our project would be to find the
        bucket size for the coarse mapper to optimize the cost of computation versus the
        cost of passing data. Our project used a rows of 5-tuples in the coarse mapper,
        which did not give us performance better than the simple or fine mapper (see
        <b>Performance</b>). Without testing more we don't know if this optimum bucket
        lies between our fine and simple mapper, between our simple and coarse mapper, or
        even larger than our coarse mapper.</p>

        <p>Because performance is highly dependent on hardware and the size of the
        dataset, one useful extension would be a system by which the size of each row
        passed to the mapper is adjusted based on the performance of previous runs.</p>

        <p>This was not within the scope of the project, but our score generator
        functions are very rudimentary. Ideally the comparison on rows such as
        <samp>subjects</samp> and <samp>courses</samp> would involve some sophisticated
        natural language processing which could run in parallel as well since there are
        no dependencies between elements in the matrix.</p>
      </div>

      <div id="tabs-7">
        <p>The biggest thing we learned from this project is that the map-reduce
        framework is not very flexible. It does not lend itself well to running
        2-variable computations when both bariables are independent, and unknown in
        advance. This made it very difficult to fill a matrix of similarity scores
        without running multiple map-reduce jobs. One way to solve this would be to
        reshape the data so that each pair of teachers has its own line, but this leads
        to a quadratic increase in space usage. There may be a way to work around this
        in MR_JOB, but if it exists it was hard to uncover. MR_JOB is definitely
        a very flexible framework, but our lack of familiarity with all but the
        most basic aspects of it made it very challenging to implement a highly
        complex system that does not seem to be in line with most map-reduce jobs.</p>

			<p>It was also a struggle to understand amazon's EC2 framework. It was straightforward to get a job running
			on EMR, but difficult to avoid the bottlenecks of booting up new instances. Ideally we would have liked to treat
			EMR as an even more powerful version of resonance, but it does not seem to be built for that. It was frustrating
			to have good ideas and working code from which we could not collect meaningful data.</p>

        <p>However, it was a lot of fun to see the tremendous speedup our implementation provided over the serial. It was also very
        satisfying to hypothesize a reason for the coarse-grained mappers slow run time and then confirm our hypothesis with new data.
        Furthermore, we were amused that the real way to speed up performance was to improve the hardware. That being said, such as increase
        in hardware benefits map-reduce much more than it might benefit the serial version which supports creating parallel algorithms whenever
        possible for a suitably large dataset.</p>
      </div>

      <div id="tabs-9">
        <p><iframe width="560" height="315" src="http://www.youtube.com/embed/9RDWeQqnWnM" frameborder="0" allowfullscreen></iframe></p>
      </div>
    </div>
  </div>
</body>
</html>